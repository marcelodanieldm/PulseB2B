# üöÄ PulseB2B - Market Intelligence Platform

**Complete market intelligence platform with multi-region serverless architecture.**

Automated pipeline that monitors business news, detects vacancies in real-time, predicts IT hiring, and generates lead scoring for global markets.

---

## üåü **NEW: Oracle Funding Detector** üîÆ

**Zero-cost AI that detects US funding and predicts hiring needs**

Parses SEC EDGAR RSS Feed for Form D filings, enriches with web scraping, and predicts hiring probability using ML - **NO API COSTS!**

### üéØ What Oracle Does
- üìÑ **SEC Form D Parser** - Auto-detects US fundraising (all venture rounds)
- üï∑Ô∏è **Smart Web Scraper** - Extracts company info + tech stacks from websites
- üß† **Hiring Probability ML** - Predicts hiring needs (0-100%) using scikit-learn
- üîç **Tech Stack Detection** - NLP keyword matching (Python, React, AWS, etc.)
- üìä **CSV Export** - Ready-to-use lead list with scores

### ‚úÖ Key Features
- **100% Free** - No paid APIs (only web scraping + NLTK + scikit-learn)
- **Smart Scoring** - 4-factor model: Funding + Tech + Intent + Recency
- **Tech Detection** - 50+ technologies across 6 categories
- **Instant Results** - 3-5 seconds per company
- **Production Ready** - Complete with logging and error handling

### üöÄ Quick Start (5 minutes)
```bash
# Windows
run_oracle.bat

# Linux/Mac
chmod +x run_oracle.sh
./run_oracle.sh

# Check results in data/output/oracle/
```

### üìñ Full Documentation
üìö **[Oracle Documentation](./docs/ORACLE_DETECTOR.md)** - Complete guide with examples

---

## üåü **Serverless Ghost Infrastructure** üëª

**Zero-cost automated market intelligence using GitHub Actions + Supabase**

Fully serverless pipeline that runs every 6 hours to detect US tech companies expanding to LATAM:

### üéØ What It Does
- üí∞ **SEC.gov RSS Scraper** - Detects US company funding (Form D filings)
- üíº **LinkedIn Jobs via Google** - Finds LATAM hiring signals (no API needed)
- üì∞ **OSINT News Analysis** - Sentiment scoring with free tools
- üéØ **Automated Lead Scoring** - 0-100 scale with priority levels
- ‚ö° **Supabase Edge Functions** - Webhooks + real-time scoring

### ‚úÖ Key Features
- **100% Free** - GitHub Actions + Supabase free tier = $0/month
- **No APIs** - Uses RSS feeds, Google Search, and public sources
- **Fully Automated** - Runs every 6 hours (4x daily)
- **Production Ready** - Complete with monitoring and notifications

### üöÄ Quick Start (15 minutes)
```bash
# 1. Install dependencies
pip install -r requirements.txt
python -c "import nltk; nltk.download('vader_lexicon')"

# 2. Setup (Windows)
.\setup_ghost.bat

# 3. Setup (Linux/Mac)
chmod +x setup_ghost.sh
./setup_ghost.sh

# 4. Follow the guide
# See docs/QUICK_START_GHOST.md for Supabase setup
```

### üìä Data Pipeline
```
GitHub Actions (Every 6 hours)
  ‚Üì
SEC.gov RSS + LinkedIn + Google News
  ‚Üì
Consolidate & Score
  ‚Üì
Supabase PostgreSQL
  ‚Üì
High Priority Leads Dashboard
```

### üìñ Documentation
- üöÄ **[Quick Start Guide](./docs/QUICK_START_GHOST.md)** - 15-minute setup
- üìö **[Complete Documentation](./docs/SERVERLESS_GHOST_INFRASTRUCTURE.md)** - Technical deep dive
- üìù **[Implementation Summary](./docs/GHOST_IMPLEMENTATION_SUMMARY.md)** - Architecture overview

### üéØ Example Queries
```sql
-- Get critical priority leads
SELECT * FROM high_priority_leads WHERE priority = 'critical';

-- Recent funding + jobs activity
SELECT * FROM recent_activity WHERE activity_date > NOW() - INTERVAL '7 days';

-- Top scoring companies
SELECT company_name, score, factors FROM lead_scores ORDER BY score DESC LIMIT 10;
```

---

## üìã Description

PulseB2B is a complete solution for market analysts who need to monitor the startup and venture capital ecosystem. The system integrates six main components:

### üéØ **Intent Classification Engine for US Tech Market (NEW!)**
1. **SEC EDGAR Scraper** - Detects new Form D filings for companies raising capital
2. **OSINT Lead Scorer** - Free sentiment analysis using GoogleNews + TextBlob/NLTK
3. **NLP Intent Classifier** - Detects outsourcing intent with HuggingFace transformers
4. **Global Hiring Score (GHS)** - Calculates offshore hiring necessity
   - Formula: `GHS = (Funding / US Median Salary) √ó Multipliers`
   - Determines if companies MUST hire offshore
   - Recommends optimal US/offshore team mix
5. **Market Orchestrator** - Unified pipeline for comprehensive analysis

**Key Features:**
- ‚úÖ 100% open-source (no paid APIs)
- ‚úÖ Heuristic scoring: Series A/B + Expansion = +50pts, Layoffs = -100pts
- ‚úÖ Keywords: 'Remote-friendly', 'Global team', 'LATAM/EMEA timezones'
- ‚úÖ Clean JSON output with hiring windows

**Quick Start:**
```bash
# Install dependencies
pip install sec-edgar-downloader GoogleNews textblob nltk

# Run setup
.\setup_intent_engine.bat  # Windows
# or
./setup_intent_engine.sh   # Linux/Mac

# Test the engine
python examples/run_intent_classification_pipeline.py
```

üìñ **[Full Documentation](./docs/INTENT_CLASSIFICATION_ENGINE.md)** | üöÄ **[Quick Start Guide](./docs/QUICK_START_INTENT_ENGINE.md)**

---

### üì∞ **News Intelligence Pipeline (Python)**
1. **Monitorea** m√∫ltiples fuentes de noticias (Google News, TechCrunch, VentureBeat, Crunchbase)
2. **Clasifica** art√≠culos seg√∫n eventos clave: Funding, Series A/B/C, Layoffs, Expansi√≥n, Adquisiciones, IPO
3. **Analiza** el sentimiento de noticias usando modelos BERT
4. **Calcula** Financial Health Scores bas√°ndose en:
   - Fecha de √∫ltima ronda de financiamiento
   - Cantidad total recaudada
   - Tama√±o de equipo
   - Eficiencia de capital
   - Sentimiento de noticias recientes

### üíº **Job Scraping System (Node.js + AWS Lambda)**
1. **Scraping Multi-Regi√≥n** con AWS Lambda en US, EU y SA
2. **Evasi√≥n de Detecci√≥n** con Playwright Stealth
3. **Rotaci√≥n de Proxies** gratuita o profesional (SmartProxy/BrightData)
4. **Watchlist Inteligente** para monitorear empresas espec√≠ficas
5. **Webhooks en Tiempo Real** v√≠a Slack, Discord, Telegram, Email
6. **Persistencia Global** con Supabase (PostgreSQL)

### ü§ñ **ML Prediction Engine (XGBoost)**
1. **Predicci√≥n de Contrataci√≥n IT** (0-100%) para pr√≥ximos 3 meses
2. **Features**: funding_recency, tech_churn, job_post_velocity, region_factor
3. **Explicabilidad SHAP** con justificaci√≥n de 3 razones por empresa
4. **Batch Processing** para an√°lisis de m√∫ltiples empresas
5. **JSON Output** con probabilidades y m√©tricas detalladas

### üéØ **Lead Scoring System (LATAM)**
1. **Web Scraping** de LinkedIn v√≠a Google Search con BeautifulSoup
2. **Hiring Potential Index (HPI)** - Score 0-100 de probabilidad de contrataci√≥n
3. **L√≥gica de Negocio**: Funding reciente + bajo crecimiento = ALTA urgencia
4. **Focus Geogr√°fico**: M√©xico y Brasil exclusivamente
5. **Reportes Autom√°ticos**: CSV con rankings y recomendaciones de acci√≥n

### üëª **Ghost System (Backend Infrastructure)**
1. **GitHub Actions como Cron** - Ejecuta scraping cada hora (gratis)
2. **Supabase Storage** - PostgreSQL cloud con cache-first logic (7 d√≠as)
3. **Webhook Notifier** - Alertas instant√°neas a Slack/Discord cuando HPI > 80%
4. **axios-retry** - Resiliencia de red con 3 reintentos exponenciales
5. **TypeScript + Node.js** - Backend robusto con validaci√≥n Zod

## üéØ Caracter√≠sticas Principales

### ÔøΩ News Intelligence (Python)

#### ÔøΩüîç Monitoreo de Noticias Multi-Fuente
- **Google News** v√≠a `pygooglenews`
- **Feeds RSS** de TechCrunch, VentureBeat, Crunchbase
- Extracci√≥n completa de contenido con `Newspaper4k`
- Deduplicaci√≥n autom√°tica de art√≠culos

### üè∑Ô∏è Clasificaci√≥n Inteligente
- Detecci√≥n de **8 categor√≠as** clave de eventos empresariales
- Sistema de scoring basado en palabras clave ponderadas
- Extracci√≥n autom√°tica de nombres de empresas
- An√°lisis de sentimiento con **DistilBERT**

#### üí∞ Financial Health Score
Algoritmo propietario que eval√∫a la salud financiera considerando:

| Componente | Peso | Descripci√≥n |
|------------|------|-------------|
| **Funding Recency** | 25% | Qu√© tan reciente fue la √∫ltima ronda |
| **Funding Amount** | 20% | Total recaudado vs. benchmarks |
| **Team Efficiency** | 20% | Relaci√≥n funding/empleado |
| **Growth Trajectory** | 15% | Tendencia de crecimiento entre rondas |
| **Funding Velocity** | 10% | Frecuencia √≥ptima de rondas |
| **News Sentiment** | 10% | An√°lisis de estabilidad en noticias |

**Score Final:** 0-100
- **80-100:** Excelente
- **65-79:** Buena
- **50-64:** Moderada
- **35-49:** Preocupante
- **0-34:** Pobre

### üíº Job Scraping System (Node.js)

#### üåç Scraping Multi-Regi√≥n Serverless
- **AWS Lambda** en 3 regiones (US-East-1, EU-West-1, SA-East-1)
- **Costo casi cero** con Free Tier de AWS
- **Ejecuci√≥n programada** cada 4-6 horas
- **Escalamiento autom√°tico** seg√∫n demanda

#### üé≠ Evasi√≥n de Detecci√≥n Avanzada
- **Playwright Stealth** con anti-detecci√≥n
- **Rotaci√≥n de User-Agents** y fingerprints
- **Simulaci√≥n de comportamiento humano**
- **Delays aleatorios** y movimientos de mouse

#### üîÑ Rotaci√≥n de Proxies Inteligente
- **Proxies gratuitos** desde APIs p√∫blicas
- **SmartProxy** integration (residencial)
- **BrightData** integration (Luminati)
- **Rotaci√≥n autom√°tica** por regi√≥n y tiempo

#### üìã Watchlist & Monitoring
- **Empresas personalizadas** para monitorear
- **Priorizaci√≥n** por importancia (1-10)
- **Scrapers especializados**: Greenhouse, Lever, Workday
- **Detecci√≥n autom√°tica** de sistemas de careers

#### üö® Webhooks en Tiempo Real
- **Notificaciones instant√°neas** de nuevas vacantes
- **M√∫ltiples canales**: Slack, Discord, Telegram, Email
- **Payloads personalizados** por empresa
- **Deduplicaci√≥n** autom√°tica de vacantes

#### üíæ Persistencia Global con Supabase
- **PostgreSQL** serverless con sync en tiempo real
- **Full-text search** de vacantes
- **Logs de scraping** detallados
- **Estad√≠sticas** por empresa y regi√≥n

## üõ†Ô∏è Tecnolog√≠as

### Backend (Python - News Pipeline)
- **Python 3.8+**
- **pygooglenews** - Acceso a Google News
- **Newspaper4k** - Extracci√≥n de contenido web
- **Transformers (HuggingFace)** - An√°lisis de sentimiento BERT
- **PyTorch** - Backend de ML
- **feedparser** - Procesamiento de RSS
- **PyYAML** - Configuraci√≥n

### Backend (Node.js - Job Scraping)
- **Node.js 18+**
- **Playwright Stealth** - Scraping con evasi√≥n
- **AWS Lambda** - Serverless computing
- **Supabase** - PostgreSQL + Auth + Storage
- **Axios** - HTTP requests para webhooks

## üì¶ Instalaci√≥n

### Prerequisitos
- **Python 3.8+** (para News Pipeline)
- **Node.js 18+** (para Job Scraping)
- **AWS CLI** configurado (para deployment)
- **AWS SAM CLI** (para serverless)
- **Cuenta Supabase** (gratis)

### 1. Clonar repositorio

```bash
git clone https://github.com/tu-usuario/PulseB2B.git
cd PulseB2B
```

### 2. Configurar Python (News Pipeline)

```bash
# Crear entorno virtual
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Descargar modelo BERT (primera vez)
python -c "from transformers import pipeline; pipeline('sentiment-analysis')"
```

### 3. Entrenar Modelo ML (Predicci√≥n de Contrataci√≥n)

```bash
# Entrenar XGBoost con datos sint√©ticos
python scripts/train_model.py

# Genera:
# - models/hiring_predictor_xgboost.pkl
# - models/hiring_predictor_rf.pkl
```

### 4. Configurar Node.js (Job Scraping)

```bash
# Instalar dependencias de Node
npm install

# Instalar Playwright browsers
npx playwright install chromium
```

### 4. Configurar Supabase

1. Crear proyecto en [supabase.com](https://supabase.com)
2. Ejecutar el schema SQL:
   ```bash
   # En Supabase Dashboard > SQL Editor
   # Copiar y ejecutar: supabase/schema.sql
   ```
3. Obtener credenciales:
   - **Project URL**: Settings > API
   - **Service Role Key**: Settings > API (secret key)

### 5. Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar .env con tus credenciales
# SUPABASE_URL=https://tu-proyecto.supabase.co
# SUPABASE_KEY=tu-service-role-key
```

### 6. Deploy a AWS Lambda (Opcional)

```bash
# Build con SAM
sam build

# Deploy (primera vez)
sam deploy --guided

# Deploy subsecuentes
sam deploy
```

**Nota:** El deploy inicial solicitar√° par√°metros:
- Supabase URL y Key
- Modo de proxy (free, smartproxy, brightdata, none)
- Webhooks de Slack/Discord (opcional)

## üöÄ Uso

### üì∞ News Intelligence Pipeline (Python)

#### Ejecutar Pipeline Completo

```bash
cd src
python main.py
```

#### Con Opciones Personalizadas

```bash
# Monitorear √∫ltimas 24 horas
python main.py --days 1

# Sin an√°lisis de sentimiento (m√°s r√°pido)
python main.py --no-sentiment

# Con archivo de configuraci√≥n personalizado
python main.py --config ../config/mi_config.yaml
```

### ü§ñ ML Prediction Engine

#### Ejecutar Predicciones

```bash
# Predecir probabilidades de contrataci√≥n
python scripts/run_predictions.py

# Genera:
# - data/predictions.json (predicciones individuales)
# - data/prediction_report.json (reporte completo)
```

#### Output Ejemplo

```json
{
  "company_name": "WorkOS",
  "prediction": {
    "probability": 87.5,
    "label": "Alta Probabilidad",
    "confidence": "Very High"
  },
  "reasons": [
    "üî• Reciente SERIES-B ($80M hace 40 d√≠as) + 3 seniors salieron = Alta probabilidad inmediata",
    "üöÄ Surge de vacantes (3.0x vs. mes anterior) con 83% de roles tech",
    "üá∫üá∏ Estados Unidos + stage Growth = Mercado competitivo (factor 1.15)"
  ],
  "features": {
    "funding_recency": 40,
    "tech_churn": 12.3,
    "job_post_velocity": 3.0,
    "region_factor": 1.15
  }
}
```

#### API Program√°tica

```python
from src.ml_predictor import HiringProbabilityPredictor
from src.feature_engineering import FeatureEngineer

# Cargar modelo
predictor = HiringProbabilityPredictor()

# Extraer features
engineer = FeatureEngineer()
features = engineer.extract_features(
    company_data={...},
    jobs_data=[...],
    funding_data=[...],
    linkedin_data={...}
)

# Predecir
prediction = predictor.predict(features)
print(f"Probability: {prediction['prediction']['probability']}%")
```

### üíº Job Scraping System (Node.js)

#### Test Local (Sin Lambda)

```bash
# Test del scraper
node scrapers/jobScraper.js

# Test de watchlist manager
node -e "const WM = require('./webhooks/watchlistManager'); new WM().getActiveCompanies().then(console.log)"
```

#### Invocar Lambda Localmente (SAM)

```bash
# Iniciar API local
sam local start-api

# En otra terminal, hacer request
curl -X POST http://localhost:3000/scrape/us-east-1 \
  -H "Content-Type: application/json" \
  -d '{"maxCompanies": 5}'
```

#### Invocar Lambda en AWS

```bash
# Via AWS CLI
aws lambda invoke \
  --function-name pulseb2b-scraper-us-east-1 \
  --payload '{"region":"us-east-1","maxCompanies":10}' \
  response.json

# Via API Gateway (despu√©s del deploy)
curl -X POST https://your-api-id.execute-api.us-east-1.amazonaws.com/Prod/scrape/us-east-1 \
  -H "Content-Type: application/json" \
  -d '{"maxCompanies": 10}'
```

#### Agregar Empresa a Watchlist

```javascript
// addCompanyToWatchlist.js
const WatchlistManager = require('./webhooks/watchlistManager');

async function main() {
  const manager = new WatchlistManager();
  
  await manager.addCompany({
    name: 'Anthropic',
    careers_url: 'https://www.anthropic.com/careers',
    scraper_type: 'greenhouse',
    region: 'us',
    priority: 10,
    webhook_url: 'https://your-webhook.com/endpoint',
    notification_channels: ['webhook', 'slack']
  });
  
  console.log('‚úì Company added to watchlist');
}

main();
```

```bash
node addCompanyToWatchlist.js
```

### üîî Configurar Webhooks

#### 1. Webhook Personalizado

```javascript
// Tu endpoint debe aceptar POST con este payload:
{
  "event": "new_jobs_detected",
  "timestamp": "2025-12-20T10:30:00Z",
  "company": {
    "id": "uuid",
    "name": "Anthropic",
    "careers_url": "https://..."
  },
  "jobs": [
    {
      "title": "Senior ML Engineer",
      "link": "https://...",
      "location": "San Francisco, CA",
      "department": "Engineering"
    }
  ],
  "summary": {
    "total_new_jobs": 5,
    "locations": ["San Francisco", "Remote"],
    "departments": ["Engineering", "Product"]
  }
}
```

#### 2. Slack Webhook

```bash
# En .env
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

Recibir√°s notificaciones formateadas:
```
üöÄ 5 New Jobs at Anthropic
‚Ä¢ Senior ML Engineer (San Francisco, CA)
‚Ä¢ Product Manager (Remote)
...
[View All Jobs] ‚Üí https://anthropic.com/careers
```

#### 3. Discord Webhook

```bash
# En .env
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/YOUR/WEBHOOK
```

#### 4. Telegram Bot

```bash
# Crear bot con @BotFather
# En .env
TELEGRAM_BOT_TOKEN=123456:ABC-DEF...
TELEGRAM_CHAT_ID=your-chat-id
```

### Usar M√≥dulos Individualmente

#### 1. Solo Scraping de Noticias

```python
from news_scraper import NewsMonitor

monitor = NewsMonitor()
articles = monitor.fetch_all_news(
    queries=["startup funding", "Series A"],
    days=7,
    extract_content=True
)

print(f"Art√≠culos obtenidos: {len(articles)}")
```

#### 2. Solo Clasificaci√≥n

```python
from news_classifier import NewsClassifier

classifier = NewsClassifier(load_sentiment_model=True)
result = classifier.classify_article(article)

print(f"Categor√≠a: {result['primary_category']}")
print(f"Sentimiento: {result['sentiment']['sentiment']}")
```

#### 3. Solo An√°lisis Financiero

```python
from financial_analyzer import FinancialHealthCalculator, CompanyData, FundingRound
from datetime import datetime

company = CompanyData(
    name="MiStartup",
    team_size=50,
    funding_rounds=[
        FundingRound("seed", 2.5, datetime(2023, 1, 15), ["Angel Investors"]),
        FundingRound("series-a", 12.0, datetime(2024, 6, 1), ["Sequoia Capital"])
    ]
)

calculator = FinancialHealthCalculator()
score = calculator.calculate_health_score(company)

print(f"Financial Health Score: {score['overall_score']}/100")
print(f"Estado: {score['health_status']}")
```

## üìä Salidas

El pipeline genera varios archivos en el directorio `data/`:

### 1. `raw_articles_YYYYMMDD_HHMMSS.json`
Art√≠culos sin procesar con metadata completa.

### 2. `classified_articles_YYYYMMDD_HHMMSS.json`
Art√≠culos clasificados con:
- Categor√≠a principal y score
- An√°lisis de sentimiento
- Empresas mencionadas

### 3. `company_insights_YYYYMMDD_HHMMSS.json`
Insights agregados por empresa:
```json
{
  "Anthropic": {
    "articles": [...],
    "categories": {"Funding": 3, "Expansion": 1},
    "avg_sentiment": {
      "positive": 0.78,
      "negative": 0.22,
      "overall": "positive"
    }
  }
}
```

### 4. `financial_scores_YYYYMMDD_HHMMSS.json`
Scores financieros detallados:
```json
{
  "company": "Anthropic",
  "health_score": {
    "overall_score": 82.5,
    "health_status": "excellent",
    "components": {...},
    "metrics": {
      "total_funding": 1154.0,
      "team_size": 150,
      "estimated_burn_rate": 1.5,
      "estimated_runway_months": 24.3
    },
    "insights": [
      "‚úì Financiamiento muy reciente (3.2 meses).",
      "‚úì Salud financiera s√≥lida en general."
    ]
  }
}
```

### 5. `report_YYYYMMDD_HHMMSS.md`
Reporte consolidado en Markdown con:
- Resumen ejecutivo
- Distribuci√≥n por categor√≠a
- Top empresas por menciones
- Financial Health Scores
- Art√≠culos destacados

### 6. `predictions.json` (ML Engine)
Predicciones de contrataci√≥n IT:
```json
{
  "company_name": "Anthropic",
  "prediction": {
    "probability": 85.2,
    "label": "Alta Probabilidad",
    "confidence": "Very High"
  },
  "reasons": [
    "üî• Reciente SERIES-C ($450M hace 65 d√≠as) indica expansi√≥n inminente",
    "üìà Velocity de vacantes 2.8x vs. mes anterior con 78% roles tech",
    "üá∫üá∏ Estados Unidos + stage Scale = Hiring continuo (factor 1.15)"
  ],
  "features": {
    "funding_recency": 65,
    "tech_churn": 8.5,
    "job_post_velocity": 2.8,
    "region_factor": 1.15
  }
}
```

### 7. `prediction_report.json` (ML Engine)
Reporte agregado con estad√≠sticas:
```json
{
  "summary": {
    "total_companies": 50,
    "high_probability": 12,
    "medium_probability": 28,
    "low_probability": 10,
    "average_probability": 58.3
  },
  "top_candidates": [...],
  "predictions": [...]
}
```

## üîß Configuraci√≥n Avanzada

### Agregar Fuentes RSS Personalizadas

```python
from news_scraper import RSSFeedSource

# En tu c√≥digo
monitor.add_source(RSSFeedSource(
    "Mi Fuente",
    "https://mifuente.com/rss"
))
```

### Personalizar Palabras Clave

Edita las categor√≠as en `src/news_classifier.py`:

```python
self.categories = {
    'Mi Categoria': {
        'keywords': ['palabra1', 'palabra2'],
        'weight': 1.2
    }
}
```

### Ajustar Pesos del Financial Health Score

En `config/config.yaml`:

```yaml
financial_health:
  weights:
    funding_recency: 0.30  # Aumentar importancia
    funding_amount: 0.15
    # ...
```

## üìà Ejemplos de Uso Real

### Caso 1: Monitoreo de Portafolio VC

```python
from main import PulseB2BPipeline
from financial_analyzer import CompanyData, FundingRound

# Mis inversiones
portfolio = [
    CompanyData(name="Startup1", team_size=45, funding_rounds=[...]),
    CompanyData(name="Startup2", team_size=120, funding_rounds=[...]),
]

pipeline = PulseB2BPipeline()
pipeline.run_full_pipeline(company_data=portfolio)
```

### Caso 2: Alertas de Riesgo

```python
# Identificar empresas en riesgo
for result in financial_scores:
    score = result['health_score']
    if score['overall_score'] < 50:
        print(f"‚ö†Ô∏è ALERTA: {result['company']}")
        print(f"Score: {score['overall_score']}")
        print(f"Runway: {score['metrics']['estimated_runway_months']} meses")
```

### Caso 3: An√°lisis de Competencia

```python
# Monitorear categor√≠a espec√≠fica
classified = pipeline.run_classification(articles)
funding_news = [a for a in classified if a['primary_category'] == 'Funding']

for article in funding_news:
    print(f"{article['title']} - {article['companies_mentioned']}")
```

## üéì Casos de Uso

- **Venture Capital:** Monitoreo de portafolio y pipeline de inversi√≥n
- **M&A:** Identificaci√≥n de targets de adquisici√≥n
- **An√°lisis Competitivo:** Tracking de competidores y mercado
- **Business Development:** Detecci√≥n de oportunidades de partnership
- **Sales Intelligence:** Identificaci√≥n de empresas en expansi√≥n

## üêõ Troubleshooting

### Error: "No module named 'torch'"

```bash
pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cpu
```

### Error: "Article Download Failed"

Algunos sitios bloquean scraping. Reduce `extract_full_content: false` en config.

### Warning: "Could not load sentiment model"

El modelo descarga ~250MB la primera vez. Verifica tu conexi√≥n a internet.

### Lentitud en clasificaci√≥n

Desactiva an√°lisis de sentimiento:
```bash
python main.py --no-sentiment
```

## üîê Consideraciones

- **Rate Limiting:** El scraper incluye delays para respetar sitios web
- **Legal:** Verifica t√©rminos de uso de cada fuente de noticias
- **Privacidad:** No almacena datos personales
- **API Keys:** No requiere API keys para funcionalidad b√°sica

## üìù Roadmap

### ‚úÖ Completado

- [x] News Intelligence Pipeline con clasificaci√≥n
- [x] Financial Health Score calculator
- [x] Multi-region serverless scraping
- [x] Webhook notifications (5 canales)
- [x] Supabase integration
- [x] **ML Prediction Engine (XGBoost + SHAP)**
- [x] **Feature engineering con 4 features principales**
- [x] **Batch predictions con reportes JSON**

### üöß En Progreso

- [ ] Integraci√≥n LinkedIn para churn real (actualmente estimado)
- [ ] Dashboard web para visualizaci√≥n de predicciones
- [ ] Fine-tuning con datos hist√≥ricos reales

### üìã Futuro

- [ ] Integraci√≥n con APIs de Crunchbase y PitchBook
- [ ] Notificaciones por email/Slack para predicciones ML
- [ ] Exportaci√≥n a Google Sheets
- [ ] An√°lisis de tendencias temporales
- [ ] Named Entity Recognition mejorado con spaCy
- [ ] Soporte para m√∫ltiples idiomas
- [ ] API REST para predicciones ML
- [ ] Integraci√≥n con CRMs (Salesforce, HubSpot)

## üìö Documentaci√≥n

- [üìñ README Principal](README.md) - Overview y setup
- [üèóÔ∏è ARCHITECTURE.md](docs/ARCHITECTURE.md) - Arquitectura del sistema
- [üöÄ DEPLOYMENT.md](docs/DEPLOYMENT.md) - Gu√≠a de deployment AWS
- [ü§ñ ML_ENGINE.md](docs/ML_ENGINE.md) - Motor de predicci√≥n ML con XGBoost
- [üéØ LEAD_SCORING.md](docs/LEAD_SCORING.md) - **Sistema de Lead Scoring para LATAM**
- [ÔøΩ Backend README](backend/README.md) - **Ghost System: GitHub Actions + Supabase**
- [üíª Frontend README](frontend/README.md) - Dashboard Next.js con Mapbox

---

## üëª Ghost System - Infraestructura Distribuida

**Sistema de scraping automatizado usando GitHub Actions como "cron jobs" gratuitos.**

### üéØ Arquitectura

```
GitHub Actions (Hourly Cron)
        ‚Üì
Python Lead Scoring Script
        ‚Üì
Node.js/TypeScript Processor
        ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì        ‚Üì
Supabase   Webhook
(Storage)  (Slack/Discord)
```

### ‚ö° Features Principales

#### 1. **GitHub Actions como Infraestructura Gratis**
- ‚úÖ Ejecuta cada hora autom√°ticamente
- ‚úÖ 2,000 minutos gratis/mes
- ‚úÖ Sin necesidad de servidores propios
- ‚úÖ Logs y artifacts incluidos

#### 2. **Cache-First Logic (7 d√≠as)**
```typescript
// No re-scraper la misma empresa m√°s de 1 vez por semana
const shouldScrape = lastScrapedAt < sevenDaysAgo;
```

#### 3. **Supabase PostgreSQL Cloud**
- ‚úÖ 500 MB storage gratis
- ‚úÖ Row Level Security
- ‚úÖ 3 tablas: `lead_scores`, `scraping_cache`, `notification_logs`

#### 4. **Webhook Notifier con Resiliencia**
```typescript
// axios-retry: 3 intentos con backoff exponencial
axiosRetry(axios, {
  retries: 3,
  retryDelay: axiosRetry.exponentialDelay // 1s, 2s, 4s
});
```

#### 5. **Notificaciones Inteligentes**
- ‚úÖ Solo empresas con HPI > 80% (configurable)
- ‚úÖ No spam: cooldown de 24h por empresa
- ‚úÖ Auto-detecta Slack o Discord
- ‚úÖ Rich formatting con todos los detalles

### üöÄ Quick Start

```bash
# 1. Install dependencies
cd backend
npm install

# 2. Configure Supabase
# Create project at https://app.supabase.com
# Run SQL schema from backend/src/supabase-client.ts

# 3. Setup webhook (Slack or Discord)
# Slack: https://api.slack.com/messaging/webhooks
# Discord: Server Settings > Integrations > Webhooks

# 4. Configure environment
cp .env.example .env
# Edit .env with your credentials

# 5. Build and test
npm run build
npm start
```

### ü§ñ GitHub Actions Setup

1. **Add Secrets** (Settings > Secrets and Variables > Actions):
   - `SUPABASE_URL`
   - `SUPABASE_ANON_KEY`
   - `WEBHOOK_URL`
   - `CRITICAL_THRESHOLD` (default: 80)

2. **Enable Workflow**: `.github/workflows/lead-scraping.yml`

3. **Monitor**: Actions tab > Lead Scoring Automation

### üìä Ejemplo de Notificaci√≥n Slack

```
üî• CRITICAL LEAD DETECTED!

Company: Kavak
Country: üá≤üáΩ Mexico
HPI Score: 85.20 (CRITICAL)
Urgency: HIGH
Employees: 200
Hiring Delta: +16 (next 6m)
Last Funding: 2024-07-20

üí° Why Critical?
‚Ä¢ Funding Recency Score: 92.50
‚Ä¢ Growth Urgency Score: 95

This lead should be contacted immediately!
```

### üîç Monitoring Queries

```sql
-- Top leads in Supabase
SELECT company_name, hpi_score, hpi_category
FROM lead_scores
ORDER BY hpi_score DESC
LIMIT 10;

-- Cache status
SELECT company_name, last_scraped_at, scrape_count
FROM scraping_cache
ORDER BY last_scraped_at DESC;

-- Notification history
SELECT company_name, hpi_score, status, created_at
FROM notification_logs
ORDER BY created_at DESC;
```

### üí° Business Logic

**L√≥gica de Urgencia**:
- Funding < 6 meses + crecimiento < 5% = **CRITICAL** (HPI boost 20%)
- Crecimiento > 20% = **LOW** (empresa saturada)

**Cache Strategy**:
- Evita re-scrapear misma empresa < 7 d√≠as
- Reduce API calls y rate limits
- Mantiene datos frescos sin desperdicio

**Notificaciones**:
- Trigger: HPI ‚â• 80% (configurable)
- Cooldown: 24h por empresa (evita spam)
- Retry: 3 intentos con exponential backoff

### üìà Costos

| Servicio | Plan | Costo |
|----------|------|-------|
| GitHub Actions | Free tier | **$0** |
| Supabase | Free tier | **$0** |
| Slack/Discord | Free | **$0** |
| **Total** | | **$0/mes** |

### üõ†Ô∏è Tech Stack

- **Node.js 20** + TypeScript 5.3
- **@supabase/supabase-js** 2.39
- **axios** + **axios-retry** 4.0
- **Zod** 3.22 (runtime validation)

Ver documentaci√≥n completa: [backend/README.md](backend/README.md)

---

## üéì Casos de Uso

- **Venture Capital:** Monitoreo de portafolio y pipeline de inversi√≥n
- **Recruiting Tech:** Predicci√≥n de empresas que contratar√°n en 3 meses
- **Lead Generation LATAM:** Identificaci√≥n de empresas en M√©xico/Brasil con alta urgencia de hiring
- **M&A:** Identificaci√≥n de targets de adquisici√≥n
- **An√°lisis Competitivo:** Tracking de competidores y mercado
- **Business Development:** Detecci√≥n de oportunidades de partnership
- **Sales Intelligence:** Identificaci√≥n de empresas en expansi√≥n

## ü§ù Contribuir

1. Fork el proyecto
2. Crea un branch (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add AmazingFeature'`)
4. Push al branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver `LICENSE` para m√°s detalles.

## üë• Autores

- **Tu Nombre** - *Desarrollo Inicial* - [GitHub](https://github.com/tu-usuario)

## üôè Agradecimientos

- HuggingFace por los modelos Transformers
- Comunidad de Python por las excelentes librer√≠as
- Todas las fuentes de noticias open-source

## üìû Contacto

Para preguntas o soporte:
- **Email:** tu-email@ejemplo.com
- **LinkedIn:** [Tu Perfil](https://linkedin.com/in/tu-perfil)
- **Issues:** [GitHub Issues](https://github.com/tu-usuario/PulseB2B/issues)

---

**Made with ‚ù§Ô∏è for Market Intelligence Professionals**
