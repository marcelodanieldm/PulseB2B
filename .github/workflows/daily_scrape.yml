name: Ghost Crawler - Daily Scrape

on:
  schedule:
    # Runs every 23 hours at random minutes to avoid detection patterns
    - cron: '17 */23 * * *'  # Every 23 hours at minute 17
  workflow_dispatch:  # Allow manual trigger for testing

env:
  NODE_VERSION: '18.x'
  PYTHON_VERSION: '3.11'

jobs:
  ghost-scrape-and-score:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      # ============================================
      # 1. CHECKOUT & SETUP
      # ============================================
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Install Python Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          echo "‚úÖ Python dependencies installed"
      
      - name: üü¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install Backend Node.js Dependencies
        run: |
          npm ci
          echo "‚úÖ Backend Node.js dependencies installed"
        working-directory: backend

      - name: üì¶ Install Frontend Node.js Dependencies
        run: |
          npm ci
          echo "‚úÖ Frontend Node.js dependencies installed"
        working-directory: frontend
      
      # ============================================
      # 2. INTELLIGENT SCRAPING (Google CSE Hack)
      # ============================================
      - name: üïµÔ∏è Ghost Crawler - LinkedIn Job Search
        env:
          GOOGLE_CSE_API_KEY: ${{ secrets.GOOGLE_CSE_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
        run: |
          echo "üîç Starting Ghost Crawler with Google Custom Search..."
          node scripts/ghost-crawler.js
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ Scraping complete"
          else
            echo "‚ö†Ô∏è Scraping encountered issues but continuing..."
          fi
      
      # ============================================
      # 3. PULSE INTELLIGENCE SCORING
      # ============================================
      - name: üß† Run Pulse Intelligence Scorer
        run: |
          echo "üß† Running Pulse Intelligence analysis..."
          
          # Check if scraped data exists
          if [ -f "data/output/scraped_companies.csv" ]; then
            python scripts/integrate_pulse_intelligence.py \
              --input data/output/scraped_companies.csv \
              --output data/output/pulse_scored.csv \
              --reports-dir data/output/pulse_reports \
              --quiet
            
            echo "‚úÖ Pulse Intelligence scoring complete"
            
            # Show quick stats
            if [ -f "data/output/pulse_reports/pulse_summary_"*.json ]; then
              echo "üìä Quick Stats:"
              python -c "import json; import glob; f=glob.glob('data/output/pulse_reports/pulse_summary_*.json')[0]; d=json.load(open(f)); print(f\"  Critical: {d['critical_opportunities']}\n  High Priority: {d['high_priority']}\n  Avg Score: {d['average_pulse_score']:.1f}\")"
            fi
          else
            echo "‚ö†Ô∏è No scraped data found, skipping scoring"
            exit 0
          fi
      
      # ============================================
      # 4. SUPABASE DATABASE SYNC (Upsert Logic)
      # ============================================
      - name: üíæ Sync to Supabase with Upsert
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          echo "üíæ Syncing scored data to Supabase..."
          
          if [ -f "data/output/pulse_scored.csv" ]; then
            node scripts/supabase-sync.js
            echo "‚úÖ Database sync complete"
          else
            echo "‚ö†Ô∏è No scored data to sync"
            exit 0
          fi
      
      # ============================================
      # 5. TELEGRAM ALERTS (90+ Score Threshold)
      # ============================================
      - name: üì¢ Send Telegram Alerts
        if: success()
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          echo "üì¢ Sending high-priority alerts..."
          
          if [ -f "data/output/pulse_reports/critical_opportunities_"*.csv ]; then
            node scripts/telegram-alerts.js
            echo "‚úÖ Alerts sent"
          else
            echo "‚ÑπÔ∏è No critical opportunities (90+ score) to alert"
          fi
      
      # ============================================
      # 6. ARTIFACT UPLOAD & CLEANUP
      # ============================================
      - name: üì§ Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ghost-crawler-results-${{ github.run_number }}
          path: |
            data/output/scraped_companies.csv
            data/output/pulse_scored.csv
            data/output/pulse_reports/
          retention-days: 7
          if-no-files-found: warn
      
      - name: üßπ Cleanup Old Artifacts
        uses: c-hive/gha-remove-artifacts@v1
        with:
          age: '7 days'
          skip-recent: 3
      
      # ============================================
      # 7. ERROR NOTIFICATION
      # ============================================
      - name: üö® Notify on Failure
        if: failure()
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          curl -s -X POST "https://api.telegram.org/bot${TELEGRAM_BOT_TOKEN}/sendMessage" \
            -d chat_id="${TELEGRAM_CHAT_ID}" \
            -d text="üö® Ghost Crawler Failed
          
          Workflow: ${GITHUB_WORKFLOW}
          Run: ${GITHUB_RUN_NUMBER}
          
          Check: https://github.com/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}" \
            -d parse_mode="HTML"
          
          echo "üö® Failure notification sent"

  # ============================================
  # 8. WEEKLY DEEP ANALYSIS (Sunday Midnight)
  # ============================================
  weekly-analysis:
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 0 * * 0'  # Only on Sundays
    needs: ghost-scrape-and-score
    
    steps:
      - name: üìä Generate Weekly Report
        run: |
          echo "üìä Generating weekly analysis..."
          # Placeholder for future weekly analytics
          echo "‚úÖ Weekly report generation (coming soon)"
